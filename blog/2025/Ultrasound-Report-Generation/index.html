<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ultrasound Report Generation: Literature Review | Ali Nikkhah </title> <meta name="author" content="Ali Nikkhah"> <meta name="description" content="You can find my projects, research interests, and sometimes personal experiences too in my blog. "> <meta name="keywords" content="engineering, Dr, PhD, Electrical Engineering"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%92&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://alinikkhah2001.github.io/blog/2025/Ultrasound-Report-Generation/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Ali</span> Nikkhah </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About Me </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Git Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Ultrasound Report Generation: Literature Review</h1> <p class="post-meta"> Created in March 17, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/vision"> <i class="fa-solid fa-hashtag fa-sm"></i> vision</a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="ultrasound-report-generation-literature-review">Ultrasound Report Generation: Literature Review</h1> <h2 id="introduction">Introduction</h2> <p>Ultrasound imaging is a widely used and safe diagnostic tool, producing real-time images without ionizing radiation. Automating the generation of ultrasound reports can ease clinician workload and reduce reporting delays (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf) (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). However, this task is challenging due to the unique characteristics of ultrasound data and medical language. Ultrasound images often have low contrast and artifacts, making it hard to extract clear visual cues (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). Moreover, ultrasound reports are typically long, detailed descriptions of organs and lesions, far more complex than short natural image captions (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). These factors create a significant <strong>vision-language gap</strong>between images and textual reports. In this review, we survey recent methods (2018–present) for ultrasound report generation, highlighting their methodologies, strengths, and limitations. We also draw parallels to general image captioning, discuss relevant computer vision (CV) and natural language processing (NLP) techniques, and examine how modern vision-language models (VLMs) can address current challenges. Finally, we explore strategies to handle fine-grained details (size, location, counts) often lost due to long-tail term distributions, and suggest future research directions.</p> <h2 id="existing-methods-for-ultrasound-report-generation">Existing Methods for Ultrasound Report Generation</h2> <p>Early approaches to automatic report generation in radiology (e.g. chest X-rays) laid the groundwork for ultrasound applications. Most methods adopt an <strong>encoder–decoder</strong> architecture inspired by image captioning (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). A Convolutional Neural Network (CNN) encodes the ultrasound image into feature representations, and a Recurrent Neural Network (RNN) (often LSTM) decodes these features into sentences (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). For example, <em>Yang et al.</em> (2021) introduced an <strong>Adaptive Multimodal Attention Network</strong> where CNN-extracted visual features are enhanced by an attention mechanism and then passed to an LSTM to generate the final ultrasound report. This attention-based CNN+LSTM approach improved relevance of generated descriptions by focusing on important image regions, demonstrating the feasibility of fully automated ultrasound report writing.</p> <p>Despite these advances, purely data-driven encoder–decoder models face difficulties. Medical images have subtle differences that non-experts struggle to distinguish, and reports must capture small abnormalities that may be visually minor (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). Models often learn to output generic or incomplete statements, missing less common findings. Researchers observed that <em>radiology</em> report generation systems trained on public X-ray datasets (IU X-Ray, MIMIC-CXR) tend to over-predict frequent normal observations while ignoring rare critical findings (<a href="https://arxiv.org/html/2409.00250v1#:~:text=long,These%20results%20highlight%20the" rel="external nofollow noopener" target="_blank">Medical Report Generation Is A Multi-label Classification Problem† Equal Contribution</a>). This <em>long-tail</em> effect is even more pronounced in ultrasound, where available datasets have limited samples of certain pathologies. In fact, until recently, ultrasound report generation was relatively under-explored compared to X-ray report generation (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). The lack of large, annotated ultrasound image–text datasets impeded progress. (To address this, some works have begun constructing dedicated ultrasound corpora; <em>Li et al.</em> built three organ-specific ultrasound image-text datasets to facilitate model training (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf).)</p> <p>A notable line of research focuses on <em>semi-automatic</em> or structured report generation. Instead of end-to-end free-text generation, these methods break the task into sub-tasks and integrate domain knowledge or user input. For instance, a 2024 study by <em>Azhar et al.</em> proposed a pipeline to synthesize <strong>structured multimodal breast ultrasound reports</strong> by combining radiologist annotations with deep learning, then uses CNN-based classifiers to detect key findings (like tumor stiffness on elastography), and finally compiles a report using predefined templates filled with the detected findings . This approach achieved high accuracy for describing suspicious breast masses and significantly reduced reporting time (to ~3.8 minutes per case) compared to fully manual writing. The strength of such methods is their reliability – by leveraging human annotations or structured templates, they virtually eliminate outright incorrect statements. However, they may produce less fluent narrative text and are limited by the need for specific inputs (e.g. annotations or measurements) from clinicians.</p> <p>In summary, existing ultrasound report generation methods can be categorized as:</p> <ul id="something-else" class="tab" data-tab="8db53580-30a6-422d-a34c-2cc88a95a982" data-name="something-else"> <li class="active" id="something-else-encoder-decoder"> <a href="#">Encoder-Decoder </a> </li> <li id="something-else-ka-model"> <a href="#">KA model </a> </li> <li id="something-else-semi-automatic"> <a href="#">Semi-Automatic </a> </li> </ul> <ul class="tab-content" id="8db53580-30a6-422d-a34c-2cc88a95a982" data-name="something-else"> <li class="active"> <ul> <li> <strong>Encoder–Decoder Generators:</strong> Fully automated models (often CNN+LSTM or Transformer-based) that learn to generate text directly from images (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). <em>Strengths:</em> end-to-end learning captures rich image-text patterns; flexible in producing free-form descriptions. <em>Weaknesses:</em> require large labeled datasets; prone to missing rare findings and making generic statements; can struggle with coherence in long reports (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf).</li> </ul> </li> <li> <ul> <li> <strong>Knowledge-Augmented Models:</strong> Encoder–decoders enhanced with extra inputs or supervision, such as known disease labels or section headings (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). For example, some radiology report models appended <em>abnormality tags</em> or used report section titles as prompts to guide the generator (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). <em>Strengths:</em> better alignment of visual features with specific report content, improving detail accuracy (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). <em>Weaknesses:</em> rely on additional annotations (e.g. labeling images with findings), which is labor-intensive and not always feasible (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf).</li> </ul> </li> <li> <ul> <li> <strong>Hybrid / Semi-automatic Systems:</strong> Multi-module pipelines that combine image analysis, information retrieval, and template-based text assembly. <em>Strengths:</em> high precision and consistency; can exploit existing structured data (measurements, annotations). <em>Weaknesses:</em> less flexible – may not describe unexpected findings beyond the template; heavier to integrate into workflow due to multiple components.</li> </ul> </li> </ul> <p>Overall, the literature after 2018 shows a trend from simple CNN-RNN models toward more sophisticated architectures that incorporate attention, multi-modal data, and external knowledge. Yet, even the best current systems have notable limitations in generating truly comprehensive and accurate ultrasound reports. Next, we discuss how insights from general image captioning research have influenced these medical applications and what additional techniques are being leveraged to overcome the challenges.</p> <h2 id="image-captioning-techniques-and-adaptation-to-medical-reports">Image Captioning Techniques and Adaptation to Medical Reports</h2> <p>The evolution of medical report generation closely parallels advances in general image captioning ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=The%20evolution%20of%20RG%20methods,Ting%20et%20al" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). Early captioning methods used <strong>retrieval-based</strong> or <strong>template-based</strong> strategies, pulling entire phrases from a database or slot-filling predefined sentence templates ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=Early%20methods%20in%20image%20captioning,Ting%20et%20al" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). While simple, those approaches struggled with new images or novel descriptions ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=template,with%20generating%20longer%2C%20more%20comprehensive" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). The advent of deep learning enabled <em>generative captioning</em>: encoding an image with CNNs and decoding text with RNNs became the dominant paradigm by the late 2010s ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=Initial%20DL%20approaches%20utilized%20CNNs,of%20VLMs%20in%20medical%20RG" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). This CNN-RNN framework, exemplified by the “Show and Tell” model, offered more flexibility to describe unseen images but still had trouble with complex scenes and longer descriptions ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=Initial%20DL%20approaches%20utilized%20CNNs,of%20VLMs%20in%20medical%20RG" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ).</p> <p>To improve fidelity, captioning researchers introduced <strong>attention mechanisms</strong> and <strong>hierarchical decoding</strong>. Attention allows the model to focus on specific image regions when generating each word, akin to how a person might look at different parts of an image while describing it. In 2015–2017, attention-based models like “Show, Attend, and Tell” significantly improved descriptive detail by learning where to look for relevant visual features. By 2018, the <em>bottom-up and top-down attention</em> approach became popular: a pretrained object detector proposes salient regions (bottom-up), and an attention LSTM decides which regions to attend to at each decoding step (top-down). These techniques help models mention specific objects (or anatomical structures, in medical images) rather than only global features.</p> <p>Medical report generation borrowed these ideas. The <em>Yang et al.</em> ultrasound model (2021) described above is one example of applying visual attention to focus on critical areas (e.g., a tumor region) before generating text ). Similarly, <em>Jing et al.</em> (2018) introduced a <strong>co-attention</strong> model for chest X-ray reports that attends to both image features and previously generated words, and crucially they used <strong>medical topic tags</strong> to guide a hierarchical text decoder (<a href="https://arxiv.org/html/2409.00250v1#:~:text=classification%20problem%20involves%20removing%20the,diseases%2C%20organs%2C%20or" rel="external nofollow noopener" target="_blank">Medical Report Generation Is A Multi-label Classification Problem† Equal Contribution</a>). In their approach, a set of disease-related tags is predicted from the image, then each tag serves as a topic for one sentence of the report. This ensured that rare yet important findings (e.g., “pneumothorax”) would be explicitly addressed by the model, rather than overlooked by a generic description. The idea of using predicted keywords or topics to steer generation is a direct adaptation of content-planning methods from NLP, tailored to the medical context.</p> <p>Another cross-pollination from general captioning is the use of <strong>reinforcement learning (RL)</strong> to optimize non-differentiable quality metrics. Traditional training maximizes likelihood of the next word, which does not always align with human preferences or task-specific metrics. In image captioning, RL (e.g. self-critical sequence training) was used to directly optimize metrics like BLEU or CIDEr. Medical works have experimented with this too. <em>Xu et al.</em> (2020) applied an RL-based training with a custom reward that penalized repetitive statements and encouraged inclusion of key findings (<a href="https://arxiv.org/html/2312.03013v1#:~:text=,07680" rel="external nofollow noopener" target="_blank">Breast Ultrasound Report Generation using LangChain</a>). By integrating an X-linear attention module and a repetition penalty, their <em>R2GenRL</em> model improved coherence and avoided common pitfalls like repeating “normal” findings multiple times. This shows how techniques initially developed to refine caption fluency and accuracy have been adapted to medical reports where redundancy and omissions can be critical issues.</p> <p>Despite these adaptations, there are important differences between captioning an everyday image and generating a medical report. A natural image caption (e.g., for MS-COCO data) is typically a single sentence about a clear scene, focusing on obvious objects and actions. In contrast, an ultrasound report is multi-sentence, often describing normal structures first, then detailing any abnormalities with precise attributes (location, size, number, etc.). The vocabulary is specialized and the required tone is clinical and factual. As a result, <strong>language models</strong> used for medical reporting often need additional domain knowledge. Recently, large pretrained models like BERT and T5 have been employed to improve the linguistic output. For example, <em>Yan and Pei</em> (2022) introduced <strong>Clinical-BERT</strong>, a vision-language pretraining approach for radiograph diagnosis that fuses CNN image features with BERT-based text encoding (<a href="https://arxiv.org/html/2312.03013v1#:~:text=,Li%2C%20Q" rel="external nofollow noopener" target="_blank">Breast Ultrasound Report Generation using LangChain</a>). Such models bring in an understanding of medical terminology and syntax, which helps generate more fluent and contextually correct reports.</p> <p>In summary, many core ideas from image captioning — CNN+RNN architectures, attention mechanisms, hierarchical generation, and reinforcement fine-tuning — have been successfully adapted for medical report generation. They provide a foundation, but medical applications often extend them with domain-specific components (like predicted medical tags or pre-trained medical language models) to meet the greater complexity of clinical descriptions. Next, we delve into specific vision and NLP techniques that have been proposed to further enhance ultrasound report generation beyond these baseline architectures.</p> <h2 id="vision-and-nlp-techniques-enhancing-ultrasound-reporting">Vision and NLP Techniques Enhancing Ultrasound Reporting</h2> <p><strong>1. Multi-modal Feature Extraction:</strong> Ultrasound data can include multiple image modes (grayscale B-mode, Doppler, elastography, etc.). Advanced systems exploit this by extracting complementary features from each mode. For instance, in breast ultrasound, <em>Azhar et al.</em> (2024) designed separate pipelines for B-mode and elastography images, using image processing to classify the scan type and deep learning to analyze mode-specific content ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11429082/#:~:text=Four%20types%20of%20breast%20US,wave%20elastography%3B%20%28d%29%20strain%20elastography" rel="external nofollow noopener" target="_blank">AI-Powered Synthesis of Structured Multimodal Breast Ultrasound Reports Integrating Radiologist Annotations and Deep Learning Analysis - PMC</a> ) ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11429082/#:~:text=,TCDL%29%20Module" rel="external nofollow noopener" target="_blank">AI-Powered Synthesis of Structured Multimodal Breast Ultrasound Reports Integrating Radiologist Annotations and Deep Learning Analysis - PMC</a> ). Fusing these multimodal features leads to richer representations of the findings. Even within a single mode, multi-scale feature extraction (e.g. using different CNN layers or multi-head attention in Transformers) can help capture both global organ structure and local lesion details. Some research also treats a series of ultrasound frames (since ultrasounds are often captured in sweeps or videos) as input, employing <strong>spatio-temporal models</strong> to incorporate the movement and continuity of ultrasound imaging (<a href="https://openreview.net/pdf?id=XKs7DR9GAK#:~:text=method%20projects%20the%20node%20features,Ultimately%2C%20a%20comprehensive%20diagnostic" rel="external nofollow noopener" target="_blank">Medical Report Generation via Multimodal Spatio-Temporal Fusion</a>). By leveraging temporal cues, models can better distinguish actual anatomical structures from transient artifacts, thus improving report accuracy.</p> <p><strong>2. Knowledge Integration:</strong> Incorporating medical knowledge explicitly has shown great promise. One approach is using <strong>knowledge graphs</strong> of medical entities. In radiology, a common practice is to define an ontology of observations (e.g. graph nodes for organs and diseases) and have the model predict which nodes are present, before generating text (<a href="https://arxiv.org/html/2409.00250v1#:~:text=resource%20for%20enhancing%20the%20quality,that%20aim%20to%20enrich%20the" rel="external nofollow noopener" target="_blank">Medical Report Generation Is A Multi-label Classification Problem† Equal Contribution</a>) (<a href="https://arxiv.org/html/2409.00250v1#:~:text=case%2C%20thereby%20focusing%20on%20the,obtain%20these%20keywords%2C%20a%20classification" rel="external nofollow noopener" target="_blank">Medical Report Generation Is A Multi-label Classification Problem† Equal Contribution</a>). This turns the task partly into a multi-label classification of findings, which can then constrain or guide the language generation. For example, <em>Zhang et al.</em> (2020) built a radiology knowledge graph with nodes for 7 organs and 20 diseases, and subsequent works expanded it; by ensuring the model covers the relevant graph nodes, they improved the completeness of reports (<a href="https://arxiv.org/html/2409.00250v1#:~:text=resource%20for%20enhancing%20the%20quality,that%20aim%20to%20enrich%20the" rel="external nofollow noopener" target="_blank">Medical Report Generation Is A Multi-label Classification Problem† Equal Contribution</a>). In ultrasound, similar strategies can be applied (e.g., a graph for fetal ultrasound might include standard anatomical landmarks and conditions). Additionally, retrieving similar past cases from the database (a form of case-based reasoning) has been used: <em>Liu et al.</em> (2019) enriched a chest X-ray generation model by retrieving sentences from reports of analogous images (<a href="https://arxiv.org/html/2409.00250v1#:~:text=resource%20for%20enhancing%20the%20quality,that%20aim%20to%20enrich%20the" rel="external nofollow noopener" target="_blank">Medical Report Generation Is A Multi-label Classification Problem† Equal Contribution</a>). This provided templates for rare findings that the model could adapt, mitigating the long-tail issue of seldom-seen phrases.</p> <p><strong>3. Advanced Vision-Language Alignments:</strong> Aligning visual and textual features is crucial, especially given the subtlety of ultrasound imagery. Recent methods explore sophisticated alignment losses and architectures. <em>Li et al.</em>(2024) introduced a <strong>Cross-Modality Feature Alignment via Unsupervised Guidance</strong> framework for ultrasound reports (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf) (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). Their key idea is to learn latent “topics” from the corpus of reports using unsupervised clustering, and then <em>guide the image encoder</em> to produce features that correspond to those topics. Concretely, they distilled “potential knowledge” from raw text (clustering recurrent patterns in reports) to serve as prior vectors (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). During training, the image features are pulled to align with the nearest text cluster (via a knowledge-matched visual extractor), ensuring that if a report topic like “thyroid nodule echogenicity” exists in text, the model learns to recognize visual cues for that topic in images (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). This reduces the mismatch between what the image provides and what the text describes. They also employed a <strong>global similarity comparison</strong> module that encourages the generated report embedding to be close to the ground-truth report embedding, fostering overall semantic alignment. Through these techniques, the model generated longer and more detailed reports with higher fidelity to the image content (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). Notably, Li et al.’s approach achieved state-of-the-art results on multiple ultrasound datasets, outperforming prior models like DeltaNet and R2GenRL by a sizable margin (e.g. +6–7 BLEU points on a breast ultrasound set) (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). It also improved recall of clinical entities in reports, meaning it mentioned more of the important findings present, without simply increasing false positives (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf).</p> <p><strong>4. NLP for Coherent and Correct Language:</strong> On the language side, ensuring the generated report is coherent, fluent, and clinically correct is a major focus. Several techniques are in play:</p> <ul> <li> <p><strong>Transformer-based Decoders:</strong> Recurrent models are increasingly being replaced or augmented by Transformers, which can capture long-range dependencies in text. Transformers enable the model to generate multi-sentence reports where early and later sentences are consistent (e.g., if the first sentence says “kidneys are normal size,” the conclusion shouldn’t contradict that). The self-attention mechanism in Transformers helps maintain context across a longer sequence than an LSTM typically can.</p> </li> <li> <p><strong>Pre-trained Language Models:</strong> Using pre-trained NLP models (BERT, GPT, etc.) for either initialization or as part of the architecture has boosted performance. <em>Yan &amp; Pei (2022)</em>’s Clinical-BERT visual-language model is one example that improved radiology report generation by starting from a model already fluent in medical text (<a href="https://arxiv.org/html/2312.03013v1#:~:text=,Li%2C%20Q" rel="external nofollow noopener" target="_blank">Breast Ultrasound Report Generation using LangChain</a>). Similarly, <em>Jin et al.</em> (2023) explored prompting a large language model with extracted findings to generate a polished report, leveraging the LLM’s knowledge to ensure factual coherence (<a href="https://arxiv.org/html/2409.00250v1#:~:text=proposed%20the%20use%20of%20medical,precise%20identification%20and%20use%20of" rel="external nofollow noopener" target="_blank">Medical Report Generation Is A Multi-label Classification Problem† Equal Contribution</a>). While LLMs (like GPT-4) are not trained on specific images, when coupled with visual feature extractors or via frameworks like LangChain, they can fill in gaps in description and use more varied vocabulary. A recent approach for breast ultrasound used an LLM agent orchestrated by LangChain to analyze images and write reports, demonstrating an ability to catch peripheral image details that previous vision-only models missed (<a href="https://arxiv.org/html/2312.03013v1#:~:text=Building%20upon%20the%20pioneering%20works,the%20generation%20of%20preliminary%20reports" rel="external nofollow noopener" target="_blank">Breast Ultrasound Report Generation using LangChain</a>) (<a href="https://arxiv.org/html/2312.03013v1#:~:text=In%20contrast%20to%20previous%20methods%2C,information%20essential%20for%20report%20generation" rel="external nofollow noopener" target="_blank">Breast Ultrasound Report Generation using LangChain</a>). This suggests a promising synergy: using the reasoning and language prowess of LLMs in combination with specialized vision models.</p> </li> <li> <p><strong>Controlled Text Generation:</strong> Ensuring the presence of critical information often means controlling the generation process. Topic or keyword-based planning (as in Jing et al. 2018) is one form of control (<a href="https://arxiv.org/html/2409.00250v1#:~:text=classification%20problem%20involves%20removing%20the,diseases%2C%20organs%2C%20or" rel="external nofollow noopener" target="_blank">Medical Report Generation Is A Multi-label Classification Problem† Equal Contribution</a>). Another form is <strong>sentence-level retrieval</strong>: <em>Zhao et al.</em> (2024) proposed a <em>Topicwise Separable Sentence Retrieval</em> method, where for each expected topic (common or rare) the system retrieves candidate sentences and then an abstractor refines them into a cohesive report (<a href="https://arxiv.org/abs/2405.04175#:~:text=increasing%20attention%20due%20to%20their,differentiated%20topics%2C%20and%20then%20propose" rel="external nofollow noopener" target="_blank">[2405.04175] Topicwise Separable Sentence Retrieval for Medical Report Generation</a>) (<a href="https://arxiv.org/abs/2405.04175#:~:text=Separable%20Sentence%20Retrieval%20,represent%20rare%20topics%20and%20establish" rel="external nofollow noopener" target="_blank">[2405.04175] Topicwise Separable Sentence Retrieval for Medical Report Generation</a>). By separating rare topics, their model specifically learns to handle uncommon findings rather than being dominated by frequent normal statements (<a href="https://arxiv.org/abs/2405.04175#:~:text=increasing%20attention%20due%20to%20their,differentiated%20topics%2C%20and%20then%20propose" rel="external nofollow noopener" target="_blank">[2405.04175] Topicwise Separable Sentence Retrieval for Medical Report Generation</a>). This controlled generation approach yielded more complete reports and set new state-of-the-art on radiology datasets, indicating its potential for ultrasound as well, where certain findings (e.g. a particular tumor type) might be very infrequent but vital to describe correctly.</p> </li> </ul> <p><strong>5. Evaluation-Guided Training:</strong> Finally, an important NLP-side technique is using improved evaluation metrics as training guidance. Traditional overlap metrics (BLEU, ROUGE) do not fully capture clinical correctness ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=However%2C%20they%20often%20fall%20short,still%20miss%20subtle%20clinical%20nuances" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). To compensate, many works use <strong>Clinical Efficacy (CE) metrics</strong> or losses. For example, one can compute if key clinical entities (lesion presence, size, etc.) appear in the generated text and optimize for that. Li et al. (2024) turned each report into a set of key entities (via expert suggestion) and measured precision/recall of those in outputs (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). Ensuring high recall of these entities during training helps the model focus on reporting all important findings. Another strategy is <em>entailment checking</em>, where a separate model evaluates if the generated report is logically entailed by (consistent with) the ground truth findings. This can catch cases where the words are correct but the context or negation is wrong. Integrating such feedback (via multitask learning or RL) pushes the generator to be factually accurate, not just linguistically similar to references.</p> <p>Through these CV and NLP innovations, recent models have become much better at generating ultrasound reports that are comprehensive and readable. Yet, significant challenges remain – particularly regarding rare or fine-grained details, which we address in the next section.</p> <h2 id="handling-fine-grained-details-and-long-tail-vocabulary">Handling Fine-Grained Details and Long-Tail Vocabulary</h2> <p>One of the toughest challenges in medical report generation is sensitivity to fine-grained details: <strong>sizes</strong>, <strong>locations</strong>, and <strong>counts</strong> of findings. These details often belong to a long-tail distribution in the training data – for example, most reports might say “no masses detected,” and only a few mention exact mass sizes or unusual locations. Models trained on such data tend to gloss over specific details, favoring generic phrasing. Ensuring that a generated report correctly captures phrases like “a 0.6×0.4×0.7 cm hypoechoic nodule in the upper pole of the right lobe” (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf) is extremely challenging.</p> <p>Researchers have proposed several strategies to mitigate this issue:</p> <ul> <li> <p><strong>Data Balancing and Augmentation:</strong> A straightforward approach is to make rare terms appear more often during training. This can involve oversampling reports with rare findings or augmenting existing reports by replacing common values with random ones. For instance, a normal-sized organ description could be augmented to create synthetic examples of enlarged organs by inserting different size adjectives. Careful augmentation can expose the model to a wider range of size and count descriptions. However, augmentation must maintain realism; simply adding random numbers risks teaching the model incorrect patterns.</p> </li> <li> <p><strong>Frequency-Normalized Learning:</strong> Some works adjust the learning algorithm to pay more attention to rare tokens. <em>Huang et al.</em> (2023) introduced a feature normalization in a graph-based model that projects knowledge graph node features such that the model’s prediction of findings becomes <em>insensitive to node frequency</em> (<a href="https://openreview.net/pdf?id=XKs7DR9GAK#:~:text=To%20mitigate%20the%20biased%20impact,see" rel="external nofollow noopener" target="_blank">Medical Report Generation via Multimodal Spatio-Temporal Fusion</a>). By mathematically re-weighting the feature space, they prevented high-frequency labels (like “normal”) from overwhelming rare ones. This yielded more balanced predictions of conditions across the long tail. Such techniques ensure that <strong>rare terms</strong> (specific sizes or uncommon pathologies) are not treated as noise but are given due importance during training (<a href="https://openreview.net/pdf?id=XKs7DR9GAK#:~:text=To%20mitigate%20the%20biased%20impact,see" rel="external nofollow noopener" target="_blank">Medical Report Generation via Multimodal Spatio-Temporal Fusion</a>).</p> </li> <li> <p><strong>Placeholder Tokens for Numbers/Locations:</strong> An innovative strategy to handle numeric and location details is to temporarily remove them from the generation problem. <em>Li et al.</em> (2024) observed that generative models struggle with precise numeric prediction given limited data. They replaced actual measurements and position terms in the training reports with special tokens (e.g., all “X cm” measurements become a generic <code class="language-plaintext highlighter-rouge">&lt;NUM&gt;</code> token) (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf) (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). The model is then trained to generate the text with placeholders. In a post-processing step, those placeholders can be filled in from an external source, such as an image analysis algorithm that measures the lesion or by copying from the input report if provided. This two-step approach acknowledges that free-form language models are not reliable for exact values. By offloading the task of getting numbers right to a dedicated measurement tool, it prevents the model from hallucinating or missing numeric details. Li et al. demonstrated this approach improved textual descriptiveness without sacrificing measurement accuracy – the final reports would include correct size values provided by another module (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf) (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf). A similar placeholder idea was used for locations (e.g., “12 o’clock position” in a breast lesion was replaced with a token), ensuring the model learns the structure of the sentence and leaves the specific location detail to be inserted separately.</p> </li> <li> <p><strong>Rare-Topic Emphasis in Training:</strong> As mentioned earlier, the <em>Topicwise Separable Retrieval</em> method explicitly separates rare topics during training (<a href="https://arxiv.org/abs/2405.04175#:~:text=increasing%20attention%20due%20to%20their,differentiated%20topics%2C%20and%20then%20propose" rel="external nofollow noopener" target="_blank">[2405.04175] Topicwise Separable Sentence Retrieval for Medical Report Generation</a>). By doing so, it forces the model to learn how to describe those rare findings well, instead of them being overshadowed. In practice, this might mean the model has a special mechanism or loss term for, say, tumor size descriptions, distinct from common normality phrases. A contrastive loss can be used to align these rare topic representations with their occurrences in the text, making the model more <em>sensitive</em> to when a rare topic should appear (<a href="https://arxiv.org/abs/2405.04175#:~:text=Separable%20Sentence%20Retrieval%20,represent%20rare%20topics%20and%20establish" rel="external nofollow noopener" target="_blank">[2405.04175] Topicwise Separable Sentence Retrieval for Medical Report Generation</a>). The result is that if an unusual phrase like “Spindle cell tumor” or “triplet pregnancy” is appropriate, the model is more likely to include it, even if it hardly ever appeared in the training data.</p> </li> <li> <p><strong>Multi-Step Generation (Detection then Description):</strong> Another way to ensure fine details are correctly mentioned is a two-step approach: first detect or classify the detail, then generate text including it. For example, one could train a model to output structured facts (like a mini-report: “Lesion count = 2; Location = upper/lower poles; Size1 = X; Size2 = Y;”) from the image, and then feed this as auxiliary input to the language model. This decomposes the problem: the vision model handles counting and measuring, tasks at which CNNs can excel with sufficient training, and the language model focuses on phrasing those facts correctly. Such <em>hybrid systems</em> can be more complex to build, but they align well with the strengths of each component and ensure that critical quantitative information is present in the final narrative.</p> </li> <li> <p><strong>Human-in-the-loop verification:</strong> In practical settings, a generated report could be audited by a clinician, but researchers have also simulated this by adding a verification model. For instance, an <em>entailment check</em> (described above) can be particularly useful for numeric and positional accuracy. If the verification model flags a possible discrepancy (say the image likely has 2 nodules but the report mentions only 1), the system could be designed to revise the report (using either a refinement model or prompting an LLM to “double-check the count”). While this strays into interactive systems, it’s a viable strategy to catch mistakes that arise from long-tail misses.</p> </li> </ul> <p>Through combinations of these strategies, recent work is making progress in overcoming the insensitivity of models to size, location, and count-related terms. It remains an open challenge, as evaluating the correctness of such details is non-trivial – it often requires pixel-level understanding or external measurement tools. Nonetheless, the trend is toward systems that either tightly integrate dedicated computer vision components for quantification or apply novel training schemes to ensure the language model gives rare but crucial terms the attention they deserve.</p> <h2 id="vision-language-models-and-future-directions">Vision-Language Models and Future Directions</h2> <p>The field is now moving toward <strong>vision-language models (VLMs)</strong> and large multimodal models to further improve report generation. Medical VLMs combine advanced computer vision backbones (often CNNs or Vision Transformers) with powerful language models (Transformers or LLMs) to create end-to-end systems that <em>understand</em> medical images and <em>articulate</em> findings ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=Medical%20vision,of%2016%20recent%20noteworthy%20medical" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ) ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=VLMs%20represent%20a%20transformative%20leap,practice%20and%20contributing%20to%20more" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). Recent reviews highlight that most existing VLMs for healthcare have targeted radiology imaging and visual QA, with fewer focused on ultrasound so far ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=VLMs%20represent%20a%20transformative%20leap,practice%20and%20contributing%20to%20more" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). However, the potential for ultrasound is significant. Below, we outline future directions and how VLMs could address current challenges:</p> <p><strong>1. Foundation Models &amp; Pretraining:</strong> Just as general AI has seen the rise of foundation models pretrained on massive data, medical imaging is starting to leverage them. Large-scale vision-language pretraining on medical data (e.g., millions of image-report pairs from hospitals) could yield a model with broad knowledge of anatomy and pathologies. For example, <em>BioViL</em> and <em>MedCLIP</em> are efforts to learn joint image-text embeddings for radiographs, which could be extended to ultrasounds by incorporating ultrasound-specific data. A powerful pretrained VLM could be fine-tuned on relatively small ultrasound datasets and still perform well, thanks to learned generalization. It might, for instance, already know the typical appearance and description of “kidney stones on ultrasound” from pretraining, even if the fine-tuning data has few examples. This addresses data scarcity and could improve recognition of long-tail conditions. Some recent work like <em>ELIXR</em> (2023) aligns a frozen language model with a radiology vision encoder to create a general-purpose medical imaging AI (<a href="https://arxiv.org/html/2312.03013v1#:~:text=of%20the%20IEEE%2FCVF%20Conference%20on,linear%20attention%20and%20repetition%20penalty" rel="external nofollow noopener" target="_blank">Breast Ultrasound Report Generation using LangChain</a>) – similar strategies can be investigated for ultrasound. The challenge here is obtaining large, diverse ultrasound datasets and computing resources for training; collaboration across institutions (or federated learning) may be key to gather enough data while maintaining privacy ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=5" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ) ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=In%20the%20future%2C%20addressing%20this,Each%20institution%20shares%20the%20updated" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ).</p> <p><strong>2. Improved Evaluation and Feedback:</strong> Future research will likely develop better evaluation metrics tailored to medical reports. Current metrics like BLEU, ROUGE, METEOR focus on surface similarity and can fail to reflect clinical significance ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=However%2C%20they%20often%20fall%20short,still%20miss%20subtle%20clinical%20nuances" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). Specialized metrics like <strong>RadGraph F1</strong> (which checks if the generated report correctly mentions clinical entities and relations) have been proposed for radiology ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=To%20address%20the%20limitations%20of,The%20development%20of%20additional" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). For ultrasound, similar metrics could be created, e.g., measuring if a report correctly captures the number of lesions, their size range, and location relations (lesion A in organ segment X). Incorporating such metrics into training (as rewards or loss components) will guide models to optimize what truly matters clinically, not just syntactic similarity ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=However%2C%20they%20often%20fall%20short,still%20miss%20subtle%20clinical%20nuances" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ) ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=To%20address%20the%20limitations%20of,The%20development%20of%20additional" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). Another direction is harnessing <strong>reinforcement learning from human feedback (RLHF)</strong>in the medical domain. Just as ChatGPT was fine-tuned with human preference data, a medical report model could be refined with feedback from radiologists about whether the report is accurate and useful. Although it’s challenging to get such high-quality feedback at scale, even a small amount could correct tendencies to hallucinate or omit important facts ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=The%20issue%20of%20hallucinations%20in,For" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ) ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=identified%20cause%20of%20hallucinations%20is,information%20from%20trusted%20textual%20sources" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ).</p> <p><strong>3. Reducing Hallucinations and Improving Trustworthiness:</strong> <em>Hallucination</em> — when a model outputs text not grounded in the image — is a serious concern in medical applications ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=The%20issue%20of%20hallucinations%20in,For" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). A future direction is to integrate <strong>retrieval-augmented generation (RAG)</strong> or factuality checks into the pipeline. RAG would allow the model to query a database of medical knowledge or reference guides when unsure, rather than guessing. For example, if an ultrasound image is unclear but certain metadata (patient history, prior reports) exists, a RAG-enabled model could pull in that context to avoid making unsupported claims. Ensuring that every statement in a generated report can be traced either to the image evidence or a trusted knowledge source will be important for clinical acceptance. Vision-language models that explicitly learn image-to-text alignment (e.g., using contrastive learning like CLIP) could also help minimize hallucination by tightly coupling what is said to what is seen ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=In%20medical%20contexts%2C%20these%20hallucinations,fabricated%20outputs%20by%20allowing%20the" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). In the near future, we might see <em>multimodal GPT-style models</em> that take the ultrasound image as input and engage in a dialog with the user (the sonographer), asking for clarification if needed and justifying each part of the generated report. Such interactivity can further increase trust, as clinicians can query the system on why it described something in a certain way.</p> <p><strong>4. Ultrasound-Specific Modeling:</strong> Ultrasound imaging has unique aspects (like operator-dependent image acquisition, various probe orientations, real-time nature) that future models should account for. One direction is <strong>view recognition</strong>and standardization – a model that first identifies the view (e.g., “four-chamber view of the heart” or “transverse thyroid view”) can then tailor its expectations and descriptions to that view. Some research on ultrasound view classification and quality assessment can be integrated so that a report generator “knows” what it’s looking at (Technology trends and applications of deep learning in ultrasonography- image quality enhancement, diagnostic support, and improving workflow efficiency.pdf) (Technology trends and applications of deep learning in ultrasonography- image quality enhancement, diagnostic support, and improving workflow efficiency.pdf). Another aspect is handling video: rather than generating a report from a single frame, models could analyze the entire sweep (a series of images) to reduce misinterpretation of transient noise. This requires efficient sequence modeling (possibly leveraging advances in video captioning and temporal attention). Additionally, <strong>3D ultrasound</strong> and <strong>automated volume scanning</strong> devices are emerging; future report generation might involve analyzing volumes, not just 2D slices . Adapting models to 3D data (using 3D CNNs or slicing the volume into key images) will be an important step as data availability grows.</p> <p><strong>5. Clinical Workflow Integration:</strong> Finally, future work should explore how these models integrate with clinicians. Rather than replacing the sonographer’s role in report writing, AI could act as an <strong>assistant</strong>. One idea is a draft generation system: the model generates an initial report which the clinician then edits. Studies show that even a semi-accurate draft can save time, as long as it’s easily correctable . This implies a need for <strong>controllable generation</strong> – the ability for a user to ask the model to adjust or regenerate a specific section (e.g., “describe the liver findings in more detail”). Achieving this might involve modular report generation (separating findings by organ system, for example) so that each part can be refined independently. Moreover, ensuring the model’s output is explainable will be crucial for trust. If the model highlights which image region led to each sentence (akin to attention heatmaps), a clinician can verify the basis of each statement. Such explainability was easier in earlier template systems; bringing it to deep learning models is a key research direction.</p> <p>In conclusion, ultrasound report generation has made significant strides since 2018, evolving from basic captioning models to sophisticated multimodal and knowledge-infused systems. By uniting advances in computer vision, NLP, and medical informatics, researchers are addressing the inherent challenges of the task – from aligning cross-modal features to handling the long tail of rare but critical terms. Vision-language models, especially large pretrained ones, are poised to play a transformative role by providing robust image understanding and fluent language generation in one package (<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=reviews%20recent%20advancements%20in%20developing,We%20also%20highlight%20current" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ) ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=VLMs%20represent%20a%20transformative%20leap,practice%20and%20contributing%20to%20more" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ). The road ahead involves not only improving model accuracy but also ensuring reliability, transparency, and seamless integration into clinical workflows. With continued interdisciplinary research, the prospect of AI-generated ultrasound reports that are as detailed and trustworthy as those written by experts is on the horizon.</p> <h2 id="references">References</h2> <ul> <li> <p>Jonghyon Yi <em>et al.</em> (2021). <strong>“Technology trends and applications of deep learning in ultrasonography: image quality enhancement, diagnostic support, and improving workflow efficiency.”</strong> <em>Ultrasonography, 40</em>(1): 7–22. <em>(Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf) (Technology trends and applications of deep learning in ultrasonography- image quality enhancement, diagnostic support, and improving workflow efficiency.pdf)</em></p> </li> <li> <p>Jun Li <em>et al.</em> (2024). <strong>“Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.”</strong> <em>arXiv preprint arXiv:2406.00644.</em> (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf) (Ultrasound Report Generation with Cross-Modality Feature Alignment via Unsupervised Guidance.pdf)*</p> </li> <li> <p>Shaokang Yang <em>et al.</em> (2021). <strong>“Automatic ultrasound image report generation with adaptive multimodal attention mechanism.”</strong> <em>Neurocomputing, 427:</em> 40–49.* (Automatic Ultrasound Image Report Generation With Adaptive Multimodal Attention Mechanism(https://www.researchgate.net/publication/346770317_Automatic_Ultrasound_Image_Report_Generation_With_Adaptive_Multimodal_Attention_Mechanism#:~:text=,))*</p> </li> <li> <p>Khadija Azhar <em>et al.</em> (2024). <strong>“AI-Powered Synthesis of Structured Multimodal Breast Ultrasound Reports Integrating Radiologist Annotations and Deep Learning Analysis.”</strong> <em>Sensors, 24</em>(17): 6055.* (Automatic Ultrasound Image Report Generation With Adaptive Multimodal Attention Mechanism(https://www.researchgate.net/publication/346770317_Automatic_Ultrasound_Image_Report_Generation_With_Adaptive_Multimodal_Attention_Mechanism#:~:text=Breast%20cancer%20is%20the%20most,Experiments%20showed%20that%20the))*</p> </li> <li> <p>Xiaosong Wang <em>et al.</em> (2018). <strong>“TieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-rays.”</strong> <em>CVPR 2018.</em> (Introduces a CNN-RNN model with an attention mechanism for chest X-ray report generation.)</p> </li> <li> <p>Bo Jing <em>et al.</em> (2018). <strong>“Automatic generation of medical imaging diagnostic report.”</strong> <em>KDD 2018.</em> (<a href="https://arxiv.org/html/2409.00250v1#:~:text=case%2C%20thereby%20focusing%20on%20the,obtain%20these%20keywords%2C%20a%20classification" rel="external nofollow noopener" target="_blank">Medical Report Generation Is A Multi-label Classification Problem† Equal Contribution</a>)*</p> </li> <li> <p>Wenbo Xu <em>et al.</em> (2020). <strong>“Reinforced medical report generation with X-linear attention and repetition penalty.”</strong> <em>arXiv:2011.07680.</em></p> </li> <li> <p>Bin Yan and Mingdong Pei (2022). <strong>“Clinical-BERT: Vision-language pre-training for radiograph diagnosis and report generation.”</strong> <em>AAAI 2022.</em> (<a href="https://arxiv.org/html/2312.03013v1#:~:text=,Li%2C%20Q" rel="external nofollow noopener" target="_blank">Breast Ultrasound Report Generation using LangChain</a>)*</p> </li> <li> <p>Junting Zhao <em>et al.</em> (2024). <strong>“Topicwise Separable Sentence Retrieval for Medical Report Generation.”</strong><em>arXiv:2405.04175.</em> (<a href="https://arxiv.org/abs/2405.04175#:~:text=increasing%20attention%20due%20to%20their,differentiated%20topics%2C%20and%20then%20propose" rel="external nofollow noopener" target="_blank">[2405.04175] Topicwise Separable Sentence Retrieval for Medical Report Generation</a>) (<a href="https://arxiv.org/abs/2405.04175#:~:text=Separable%20Sentence%20Retrieval%20,represent%20rare%20topics%20and%20establish" rel="external nofollow noopener" target="_blank">[2405.04175] Topicwise Separable Sentence Retrieval for Medical Report Generation</a>)*</p> </li> <li> <p>Zexue Huang <em>et al.</em> (2023). <strong>“RadFusion: Radiology Report Generation via Graph-integrated Multimodal Fusion.”</strong> <em>MICCAI 2023 (Lecture Notes in Computer Science, vol. 14233).</em> (<a href="https://openreview.net/pdf?id=XKs7DR9GAK#:~:text=To%20mitigate%20the%20biased%20impact,see" rel="external nofollow noopener" target="_blank">Medical Report Generation via Multimodal Spatio-Temporal Fusion</a>)*</p> </li> <li> <p>Iryna Hartsock and Ghulam Rasool (2024). <strong>“Vision-language models for medical report generation and visual question answering: a review.”</strong> <em>Frontiers in Artificial Intelligence, 7:1219193.</em> ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=Initial%20DL%20approaches%20utilized%20CNNs,of%20VLMs%20in%20medical%20RG" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> ) ( <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC11611889/#:~:text=To%20address%20the%20limitations%20of,The%20development%20of%20additional" rel="external nofollow noopener" target="_blank">Vision-language models for medical report generation and visual question answering: a review - PMC</a> )*</p> </li> <li> <p>Additional references are embedded inline above, marked with the <strong>【†】</strong> notation for specific supporting sources.</p> </li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2015/disqus-comments/">a post with disqus comments</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2015/code/">a post with code</a> </li> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Ali Nikkhah. Last updated: March 19, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/tabs.min.js?b8748955e1076bbe0dabcf28f2549fdc"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>