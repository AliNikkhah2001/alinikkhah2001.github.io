<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="1-dataset-preprocessing-and-image-normalization">1. Dataset Preprocessing and Image Normalization</h2> <table> <tbody> <tr> <td>(<a href="">image</a>) *Various inpainting methods for removing textual annotations from ultrasound images. The baseline (left) shows markers and text (“+ BPD” for bi-parietal diameter). Successive examples show removal via masking (black boxes), noise fill-in, patch-based matching, interpolation, a GAN-based method, and a U-Net inpainting model ([Illustration of inpainting methods replacing both text and calipers on…</td> <td>Download Scientific Diagram](https://www.researchgate.net/figure/llustration-of-inpainting-methods-replacing-both-text-and-calipers-on-two-example-image_fig1_369540609#:~:text=Confounding%20information%20in%20the%20form,overrepresented%20in%20images%20of%20malignant)) ([Illustration of inpainting methods replacing both text and calipers on…</td> <td>Download Scientific Diagram](https://www.researchgate.net/figure/figure/llustration-of-inpainting-methods-replacing-both-text-and-calipers-on-two-example-image_fig1_369540609/actions#embed#:~:text=Fig%202%20,information%20from%20fetal%20ultrasound%20images)).*</td> </tr> </tbody> </table> <table> <tbody> <tr> <td> <strong>Removing Text/Markers with Generative Inpainting:</strong> Modern approaches treat the removal of text, calipers, and other non-anatomical markings as an image inpainting problem. Deep generative models like GANs and diffusion models have been successful in <strong>blind inpainting</strong> – filling in masked regions without explicit location annotations. For example, a Pyramid GAN was used to remove crosshair markers in thyroid ultrasound images by leveraging multi-scale context (<a href="https://www.sciencedirect.com/science/article/abs/pii/S001048252100545X#:~:text=,in%20thyroid%20ultrasound%20images" rel="external nofollow noopener" target="_blank">Automatic consecutive context perceived transformer GAN for serial …</a>). Diffusion models with appropriate conditioning (e.g. using ControlNet or masked diffusion) can similarly erase text overlays by treating text removal as a conditional generation task (<a href="https://arxiv.org/abs/2410.21721#:~:text=DiffSTR%3A%20Controlled%20Diffusion%20Models%20for,develop%20a%20mask%20pretraining" rel="external nofollow noopener" target="_blank">DiffSTR: Controlled Diffusion Models for Scene Text Removal - arXiv</a>). These methods learn to synthesize realistic tissue texture where the text or artifact was, producing a clean image. In practice, they outperform naive fixes (like blurring or cropping) by preserving underlying anatomy. Recent surveys note that image inpainting has been widely applied to <strong>text removal</strong> and watermark deletion in images (<a href="https://www.diva-portal.org/smash/get/diva2:1637812/FULLTEXT01.pdf#:~:text=In%20image%20restoration%2C%20image%20inpainting,the%20original%20text%20in%20an" rel="external nofollow noopener" target="_blank"></a>), and medical adaptations have emerged to handle radiologist markings. When deep learning resources are limited, classical inpainting techniques are still relevant: methods like patch-based filling (e.g. PatchMatch) or partial convolution can propagate neighboring pixel intensities into text regions. A morphology-based strategy by Alex <em>et al.</em> solves a Laplacian equation on the image domain to seamlessly fill marker regions ([Morphology Based Inpainting For Removal Of Markers In Medical Images</td> <td>Request PDF](https://www.researchgate.net/publication/340583310_Morphology_Based_Inpainting_For_Removal_Of_Markers_In_Medical_Images#:~:text=in%20wide%20range%20of%20image,for%20automatic%20disease%20diagnosis%20interpretation)) – essentially a fast interpolation guided by surrounding pixel values. Such traditional approaches (illustrated by the “FastMatching” and “Interpolation” examples above) can remove simple annotations without learning, although they may struggle with large or complex obstructions.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td> <strong>Standardizing Image Size and Shape:</strong> Ultrasound images often come in varying sizes or with irregular “fan” shapes (sector scan regions) and padding. To feed data consistently to deep models, preprocessing pipelines use transformations to normalize geometry. A straightforward approach is to <strong>pad and resize</strong> all images to a fixed resolution, preserving aspect ratio by adding black borders. However, more advanced techniques avoid over-cropping or warping important anatomy. <strong>Spatial transformer networks (STN)</strong> can automatically learn to rotate, scale, or crop images to a canonical view ([Automatic Facial Axes Standardization of 3D Fetal Ultrasound Images</td> <td>SpringerLink](https://link.springer.com/content/pdf/10.1007/978-3-031-73647-6_9#:~:text=3D%20US%2C%20reducing%20sonographer%20workload,observer)). For example, an STN module can be trained to detect the organ of interest and align it to the center of the frame, without any manual ROI annotations. In 3D fetal ultrasound, applying a learned rotation/translation via an STN significantly reduced orientation variability across scans ([Automatic Facial Axes Standardization of 3D Fetal Ultrasound Images</td> <td>SpringerLink](https://link.springer.com/content/pdf/10.1007/978-3-031-73647-6_9#:~:text=3D%20US%2C%20reducing%20sonographer%20workload,observer)). Another strategy is <strong>coordinate warping</strong> based on ultrasound physics: Ramakrishnan <em>et al.</em> convert fan-shaped scans to a rectangular grid by re-sampling along the polar angle, apply standard augmentations, then warp back (<a href="https://openaccess.thecvf.com/content/CVPR2024W/DCAMI/papers/Ramakers_UltraAugment_Fan-shape_and_Artifact-based_Data_Augmentation_for_2D_Ultrasound_Images_CVPRW_2024_paper.pdf#:~:text=2,we%20refer%20to%20section%205" rel="external nofollow noopener" target="_blank">UltraAugment: Fan-shape and Artifact-based Data Augmentation for 2D Ultrasound Images</a>). This kind of distortion-aware preprocessing ensures that all images share a common grid while retaining the real content. In summary, combining automated cropping (or STN alignment) with appropriate padding yields uniform image inputs without requiring predefined crop regions. These steps — generative inpainting to clean artifacts and learned transformations to normalize shape — produce a high-quality, standardized dataset as a foundation for report generation models.</td> </tr> </tbody> </table> <h2 id="2-unsupervised-report-clustering-and-topic-discovery">2. Unsupervised Report Clustering and Topic Discovery</h2> <p><strong>Clustering and Topic Modeling of Textual Reports:</strong> Unsupervised learning can structure free-text ultrasound reports into meaningful groups, which helps distill common findings or narratives. A typical pipeline is to first embed each report into a numerical vector space (using techniques like TF-IDF, doc2vec, or Sentence-BERT) and then apply clustering on these embeddings (<a href="file://file-GiuhXaYtmoSYoqVxzm2BVq#:~:text=evaluated%20three%20report%20embedding%20methods%2C,bowman2015large%2C%20williams2018broad">Methodology.txt</a>) (<a href="file://file-GiuhXaYtmoSYoqVxzm2BVq#:~:text=%5Csubsubsection,assigned%20to%20the%20vector%20%24y_i">Methodology.txt</a>). For instance, in one approach the text of ultrasound reports was converted to embeddings (Sentence-BERT yielded good representations for long medical text), reduced in dimensionality (via UMAP), and then clustered with <em>k</em>-means to uncover $K$ latent topics (<a href="file://file-GiuhXaYtmoSYoqVxzm2BVq#:~:text=evaluated%20three%20report%20embedding%20methods%2C,bowman2015large%2C%20williams2018broad">Methodology.txt</a>) (<a href="file://file-GiuhXaYtmoSYoqVxzm2BVq#:~:text=%5Csubsubsection,assigned%20to%20the%20vector%20%24y_i">Methodology.txt</a>). Each cluster corresponds to a set of reports with similar content – for example, one cluster might group reports discussing cysts and another normal anatomy. Wang <em>et al.</em> demonstrate at a large scale (nearly 1.9 million radiology reports) that such unsupervised clustering can indeed identify the underlying <strong>major topics</strong> present in the corpus (<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC4722022/#:~:text=developed%20an%20unsupervised%20machine%20learning,a%20domain%20expert%20radiologist%20and" rel="external nofollow noopener" target="_blank"> Unsupervised Topic Modeling in a Large Free Text Radiology Report Repository - PMC </a>). Their method used cosine similarity of TF-IDF vectors to cluster reports and discovered around 19 coherent topics (verified by a radiologist) without any prior labels (<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC4722022/#:~:text=developed%20an%20unsupervised%20machine%20learning,a%20domain%20expert%20radiologist%20and" rel="external nofollow noopener" target="_blank"> Unsupervised Topic Modeling in a Large Free Text Radiology Report Repository - PMC </a>). Likewise, <strong>topic modeling</strong> algorithms like Latent Dirichlet Allocation (LDA) have been applied to radiology text to automatically learn themes such as “lesion description,” “normal exam,” or “Doppler findings.” The key benefit is creating a structured representation (topic or cluster IDs) for each report, which can serve as high-level labels or “prior knowledge” in a multi-modal model.</p> <p><strong>Linking Image Regions to Text Clusters:</strong> Once report clusters or topics are defined, a next-level challenge is associating aspects of the images with those topics. Recent research in multi-modal learning uses <strong>weakly supervised alignment</strong> techniques to connect textual patterns with image regions. For example, a model can learn attention weights that highlight which part of an ultrasound image corresponds to a particular cluster’s content. In one ultrasound report generation framework, the clustered “knowledge” from texts was used to guide a visual feature extractor – essentially treating each cluster as a pseudo-label and training the CNN to recognize visual features indicative of that cluster (<a href="file://file-GiuhXaYtmoSYoqVxzm2BVq#:~:text=acquired%20by%20the%20knowledge%20distiller,between%20visual%20and%20textual%20features">Methodology.txt</a>). This yields an alignment where certain image patterns activate specific cluster neurons. Attention-based architectures can also create explicit linkages: by using co-attention or cross-modal attention, the model can attend to a region of the image when generating a phrase (or when classifying a report into a topic). For instance, an attention map from an aligned model might focus on the region around the liver when the report topic involves “hepatic lesions.” Indeed, Zhang <em>et al.</em> visualized word-level attention for generated reports and found that terms like “liver” or “bile duct” strongly attend to the correct anatomical area in the ultrasound image (<a href="file://file-Jw5mXipjPmKNcsm5tLUcfP#:~:text=Furthermore%2C%20we%20visualize%20the%20attention,processes%20information%20at%20each%20word">Results.txt</a>). This suggests the model learned an implicit mapping between those words (and by extension the cluster of “hepatobiliary findings”) and image regions. Another approach is to use <strong>contrastive learning</strong> across image patches and sentence embeddings: by clustering sentence descriptions (e.g. “thyroid nodule characteristics”) and training the image encoder to produce similar embeddings when that finding is present, the network can localize which image patches correlate with each description cluster. Liu <em>et al.</em> (2024) introduce a multi-grained method that extracts sentence-level topics from reports and contrasts each sentence embedding against the image features, forcing the model to distinguish different topics within the image (<a href="https://pubmed.ncbi.nlm.nih.gov/38437149/#:~:text=incorporating%20sentence,to%20first%20generate%20coarse%20sentence" rel="external nofollow noopener" target="_blank">Multi-Grained Radiology Report Generation With Sentence-Level Image-Language Contrastive Learning - PubMed</a>). This kind of fine-grained alignment ensures that if, say, one cluster/topic is “vascularity on Doppler,” the model will learn to look specifically at color Doppler regions for that topic. In summary, unsupervised textual clustering (using methods like k-means or LDA) yields high-level groupings of report content, and new deep learning techniques can associate those groupings with image evidence. This forms a bridge between recurring textual themes and consistent visual cues in ultrasound exams, all without explicit region annotations.</p> <h2 id="3-alternative-loss-functions-and-model-improvements">3. Alternative Loss Functions and Model Improvements</h2> <p><strong>Beyond Triplet Loss – Contrastive and Self-Supervised Learning:</strong> Many current ultrasound report generators use combined objectives (e.g. image-text matching losses like triplet loss). Rather than tuning the weights of existing loss terms, researchers are exploring fundamentally different loss functions to improve representation learning and generation quality. One powerful class is <strong>contrastive losses</strong>. Contrastive learning pulls matched image–text pairs together in the feature space and pushes mismatched pairs apart. This can replace or augment triplet loss with a more global batch-based objective. A prominent example is the ConVIRT framework, which pretrains medical image encoders using a bidirectional image-text contrastive loss (<a href="https://arxiv.org/abs/2010.00747#:~:text=similarity,as%20much" rel="external nofollow noopener" target="_blank">[2010.00747] Contrastive Learning of Medical Visual Representations from Paired Images and Text</a>). In ConVIRT, each report serves as a “positive” description for its associated image and as a “negative” for all other images in the batch, encouraging the model to learn fine-grained visual features that align with the text. The result was state-of-the-art visual representations that outperformed even ImageNet pretraining for downstream tasks (<a href="https://arxiv.org/abs/2010.00747#:~:text=descriptive%20text,as%20much" rel="external nofollow noopener" target="_blank">[2010.00747] Contrastive Learning of Medical Visual Representations from Paired Images and Text</a>). Applying such a contrastive loss in ultrasound means the model learns a rich alignment between ultrasound images and report text without any explicit labels – essentially a self-supervised grounding of semantics. Beyond image-text contrast, <strong>self-supervised losses</strong> on individual modalities can also help. For images, one could incorporate losses for predicting transformations (e.g. rotation, inversion) or masked patch reconstruction (similar to Masked Autoencoders) to ensure the visual encoder learns robust features. For text, language-modeling losses (predicting masked words or next sentence) on the corpus of reports could be added. These self-supervised signals imbue the model with domain-specific understanding (e.g. how ultrasound images typically look, or how reports are written) that goes beyond the paired training data. In practice, combining contrastive objectives with such pretext tasks can yield a more informative shared embedding space, improving the subsequent report generation.</p> <p><strong>Reinforcement Learning for Report Generation:</strong> Another advanced direction is formulating report generation as a reinforcement learning (RL) problem, where the model’s “action” is to output a sequence of words and a reward function measures the quality or accuracy of the report. The standard training of image-to-text models uses cross-entropy loss against ground truth sentences; however, this can be augmented with RL to directly optimize non-differentiable metrics or long-term consistency. For example, Qin and Song (2022) introduced an RL agent that learns to align visual and textual features via a reward signal from a cross-modal memory (<a href="https://aclanthology.org/2022.findings-acl.38/#:~:text=mechanism%20under%20supervised%20settings%2C%20they,two%20English%20radiology%20report%20datasets" rel="external nofollow noopener" target="_blank">Reinforced Cross-modal Alignment for Radiology Report Generation - ACL Anthology</a>). Their model stored mappings between image regions and text tokens, and an RL algorithm adjusted the generation policy to improve the alignment (even though no ground truth alignment was given) (<a href="https://aclanthology.org/2022.findings-acl.38/#:~:text=propose%20an%20approach%20with%20reinforcement,modal" rel="external nofollow noopener" target="_blank">Reinforced Cross-modal Alignment for Radiology Report Generation - ACL Anthology</a>). This yielded better feature correspondence and more accurate descriptions. RL can also use external metrics as feedback. One study used a clinical fact accuracy metric (RadGraph’s F1 score) as part of the reward to penalize factual errors in generated chest X-ray reports (<a href="https://www.mdpi.com/2306-5354/11/4/351#:~:text=introduces%20reinforcement%20learning%20and%20text,1%7DCheXbert%2C%20and%20RadGraph%2C%20setting%20new" rel="external nofollow noopener" target="_blank">Improving Radiology Report Generation Quality and Diversity through Reinforcement Learning and Text Augmentation</a>). By <strong>employing a knowledge-guided reward</strong>, they significantly improved the correctness of outputs – the model learned, for instance, not to mention a finding if the image (and thus the reward computation) indicated it was absent (<a href="https://www.mdpi.com/2306-5354/11/4/351#:~:text=introduces%20reinforcement%20learning%20and%20text,1%7DCheXbert%2C%20and%20RadGraph%2C%20setting%20new" rel="external nofollow noopener" target="_blank">Improving Radiology Report Generation Quality and Diversity through Reinforcement Learning and Text Augmentation</a>). Similarly, researchers have applied the self-critical sequence training approach from image captioning, where the reward is based on domain metrics like BLEU or specialized medical scores. Parres <em>et al.</em> (2024) combined a vision-transformer-based model with RL and data augmentation, using the CheXpert and RadGraph metrics as rewards to enhance both language quality and clinical accuracy (<a href="https://www.mdpi.com/2306-5354/11/4/351#:~:text=,quality%2C%20factual%20correctness%2C%20and%20completeness" rel="external nofollow noopener" target="_blank">Improving Radiology Report Generation Quality and Diversity through Reinforcement Learning and Text Augmentation</a>) (<a href="https://www.mdpi.com/2306-5354/11/4/351#:~:text=metrics%3A%20RadGraph%20,quality%20reports" rel="external nofollow noopener" target="_blank">Improving Radiology Report Generation Quality and Diversity through Reinforcement Learning and Text Augmentation</a>). The use of RL led to new state-of-the-art performance on multiple metrics, indicating the model learned to prioritize important clinical content. In the ultrasound domain, one could define rewards for mentioning key findings (or avoiding contradictions) and let the generator gradually improve itself. It’s worth noting that RL is often used in tandem with other losses: for example, a recent multi-topic report generator first uses contrastive learning to better separate normal vs abnormal features, then fine-tunes the language decoder with reinforcement learning to boost clinically crucial details (<a href="https://pubmed.ncbi.nlm.nih.gov/38437149/#:~:text=sentence%20contents%20and%20refined%20image,art%20methods" rel="external nofollow noopener" target="_blank">Multi-Grained Radiology Report Generation With Sentence-Level Image-Language Contrastive Learning - PubMed</a>). Overall, incorporating objectives like contrastive alignment and reinforcement learning moves beyond static triplet loss weighting. These techniques allow the model to <strong>learn from the structure of the data itself</strong> (via self-supervision) and from <strong>task-specific feedback signals</strong> (via RL), leading to more robust representations and more accurate, coherent report generation.</p> <h2 id="4-ensuring-medical-accuracy-and-consistency">4. Ensuring Medical Accuracy and Consistency</h2> <p><strong>Automated Fact-Checking with Knowledge Graphs and Rules:</strong> To verify the medical accuracy of generated reports, researchers are turning to explicit knowledge-based systems. One approach is to use a <strong>medical knowledge graph (KG)</strong> – a structured database of medical facts (e.g. anatomical entities, pathologies, and their relationships) – as a reference for checking the report. The generated report can be parsed into a set of facts using information extraction tools (like RadGraph for radiology, which identifies clinical entities and relations in text). Those facts are then compared against the knowledge graph or checked for plausibility. For example, if a report states “the thyroid nodule is 5 cm with benign features,” an automated checker could verify that a 5 cm thyroid nodule is within a plausible size range and that “benign features” is not contradictory to any described ultrasound characteristics. Techniques exist to convert free-text reports into structured graphs of findings: Zhang <em>et al.</em> (2024) introduce a system “ReXKG” that takes radiology reports and constructs a comprehensive knowledge graph of their content (<a href="https://arxiv.org/abs/2408.14397#:~:text=evaluation%20methods%20fail%20to%20reveal,Our%20study%20provides%20a" rel="external nofollow noopener" target="_blank">[2408.14397] Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs</a>). By comparing the structure of an AI-generated report’s graph to that of typical human reports or to known medical ontologies, one can spot omissions or anomalies (e.g. missing a description of a nodule’s location, or an impossible finding given the organ). Rule-based systems are a more direct way to enforce domain knowledge. In fact, <strong>rule-based expert systems</strong> have a long history in medicine – MYCIN in the 1970s encoded hundreds of rules for diagnosing infections (<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9259200/#:~:text=Analysis%20pmc.ncbi.nlm.nih.gov%20%20MYCIN%20,Many" rel="external nofollow noopener" target="_blank">Knowledge Graph Applications in Medical Imaging Analysis</a>). Today’s rule-based verification might involve a set of if-then checks: for instance, if the report mentions “gallbladder: normal” it should not also mention gallstones, or if a measurement is given it must be within human physiological limits. These rules can be derived from clinical guidelines or consistency criteria used by radiologists. While no rule set can cover every scenario, even a small library of rules can catch blatant errors (like contradicting statements or missing critical components such as an impression for an abnormal finding). Knowledge graphs and rules often work together – the KG provides a schema of what relationships are expected (lesion <em>found in</em> organ, size <em>property of</em> lesion, etc.), and rule logic checks that the report’s content fits that schema and medical realism (<a href="https://aclanthology.org/2023.acl-industry.2.pdf#:~:text=7,3%20Related%20Work" rel="external nofollow noopener" target="_blank"></a>) (<a href="https://aclanthology.org/2023.acl-industry.2.pdf#:~:text=8,Loveymi%20et" rel="external nofollow noopener" target="_blank"></a>). If a generated report violates these constraints, it can be flagged for review or corrected.</p> <p><strong>Secondary Model Verification and Expert-in-the-Loop:</strong> Another line of defense is using independent AI models or classifiers to validate the report’s content against the images. In this setup, after a report is generated, a secondary model (which could be an image classifier or detector trained on specific pathologies) examines the ultrasound image and predicts key findings. Its predictions (for example, “thyroid nodule present” or “no nodule”) are then compared to the text. Discrepancies trigger a warning – e.g. the image model sees a mass but the report fails to mention it, or vice versa. This acts as an automated double-reading: one model generates the report, another checks it. For instance, in chest X-ray report generation research, a common practice is to use a pre-trained disease classifier to compute metrics like CheXpert accuracy for the report (<a href="https://www.mdpi.com/2306-5354/11/4/351#:~:text=from%20SwinB%2BBERT9k%20and%20their%20corresponding,U" rel="external nofollow noopener" target="_blank">Improving Radiology Report Generation Quality and Diversity through Reinforcement Learning and Text Augmentation</a>). Essentially, the report is fed into a labeler that extracts findings (normal/abnormal labels), and those are checked against ground truth labels or image-based labels. The same concept can be applied to ultrasound: a classifier could verify the presence of common findings such as lesions, calcifications, or certain Doppler signals. If the report says “no tumor” but a tumor-detection network finds a suspicious region, the conflict indicates a likely error in the report. Apart from fully automated checks, an <strong>expert-in-the-loop</strong> can be incorporated at critical points. In a practical deployment, the AI-generated ultrasound report would be reviewed by a radiologist or sonographer who can correct any inaccuracies. While the ultimate goal is automation, this human oversight is invaluable for patient safety. Some frameworks even allow iterative refinement, where the expert’s feedback is fed back to the model to improve future outputs. Kottler (2024) emphasizes that an expert-in-the-loop paradigm ensures collaboration between AI and clinicians, especially at higher autonomy levels of report generation where the AI might draft the report and the human finalizes it (<a href="https://www.i-jmr.org/2022/2/e38655#:~:text=Levels%20of%20Autonomous%20Radiology%20In,software%20developers%2C%20and%20expert%20radiologists" rel="external nofollow noopener" target="_blank">Levels of Autonomous Radiology</a>) (<a href="https://ieeexplore.ieee.org/iel8/4664312/10856212/10746601.pdf#:~:text=r%20Expert,While" rel="external nofollow noopener" target="_blank">Artificial General Intelligence for Medical Imaging Analysis</a>). In summary, ensuring medical accuracy involves multiple layers: <strong>knowledge-driven verification</strong> (graphs and rules that encode medical consistency), <strong>data-driven verification</strong> (secondary models and consistency metrics), and <strong>human oversight</strong> as a safety net. By combining these, the system can automatically catch many factual errors or omissions in ultrasound reports. For example, a rule/graph might catch that a “kidney stone” is mentioned without any mention of the kidney’s appearance (an inconsistency), a secondary model might catch that the image actually shows a stone despite the report not mentioning it, and finally a radiologist can correct subtle issues that automation misses. This multi-pronged approach greatly enhances the reliability of automatically generated reports, moving us closer to safe clinical adoption.</p> <p><strong>Sources:</strong> The content above integrates insights from recent literature on medical image inpainting (<a href="https://www.sciencedirect.com/science/article/abs/pii/S001048252100545X#:~:text=,in%20thyroid%20ultrasound%20images" rel="external nofollow noopener" target="_blank">Automatic consecutive context perceived transformer GAN for serial …</a>) (<a href="https://www.researchgate.net/publication/340583310_Morphology_Based_Inpainting_For_Removal_Of_Markers_In_Medical_Images#:~:text=in%20wide%20range%20of%20image,for%20automatic%20disease%20diagnosis%20interpretation" rel="external nofollow noopener" target="_blank">Morphology Based Inpainting For Removal Of Markers In Medical Images | Request PDF</a>), unsupervised clustering of radiology reports (<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC4722022/#:~:text=developed%20an%20unsupervised%20machine%20learning,a%20domain%20expert%20radiologist%20and" rel="external nofollow noopener" target="_blank"> Unsupervised Topic Modeling in a Large Free Text Radiology Report Repository - PMC </a>), advanced loss functions for vision-language models (<a href="https://arxiv.org/abs/2010.00747#:~:text=similarity,as%20much" rel="external nofollow noopener" target="_blank">[2010.00747] Contrastive Learning of Medical Visual Representations from Paired Images and Text</a>) (<a href="https://aclanthology.org/2022.findings-acl.38/#:~:text=mechanism%20under%20supervised%20settings%2C%20they,two%20English%20radiology%20report%20datasets" rel="external nofollow noopener" target="_blank">Reinforced Cross-modal Alignment for Radiology Report Generation - ACL Anthology</a>), and medical fact-checking in radiology report generation (<a href="https://www.mdpi.com/2306-5354/11/4/351#:~:text=introduces%20reinforcement%20learning%20and%20text,1%7DCheXbert%2C%20and%20RadGraph%2C%20setting%20new" rel="external nofollow noopener" target="_blank">Improving Radiology Report Generation Quality and Diversity through Reinforcement Learning and Text Augmentation</a>) (<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9259200/#:~:text=Analysis%20pmc.ncbi.nlm.nih.gov%20%20MYCIN%20,Many" rel="external nofollow noopener" target="_blank">Knowledge Graph Applications in Medical Imaging Analysis</a>). These techniques and studies collectively highlight how to improve ultrasound report generation pipelines – from cleaner input images and learned textual structures, to novel training losses and rigorous validation of the generated narratives. Each component addresses a key challenge (visual noise, data heterogeneity, training signal, and output accuracy), and together they push automatic ultrasound reporting closer to expert-level performance.</p> </body></html>