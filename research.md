---
layout: page
title: Research
permalink: /research/
---

I'm a **machine learning researcher / engineer** working across affective computing, medical reasoning, and agentic marketplaces. This notebook highlights the programs and questions I am actively exploring. My practice balances rigorous experimentation with deployable systems that partner with clinicians, linguists, and product leaders.

I'm highly motivated to pursue master's or PhD studies in these areas and I'm always open to co-authoring or supporting labs that need reproducible infrastructure. If any of the directions below resonate, please [send me a note](/contact/).

## Emotion-aware speech translation
- **Lab:** Trinity College Dublin · EmoDub Project
- **Focus:** Preserving affect during multilingual speech-to-speech translation.
- Engineered speech emotion recognition front-ends that blend wav2vec2.0, Whisper, and transformer SER encoders.
- Built multimodal fusion stacks to align acoustic, semantic, and contextual signals for affect-consistent decoding.
- Evaluated BLEU, Emotion F1, and MOS to quantify emotional fidelity alongside translation accuracy.

## Ultrasound report generation
- **Lab:** University of British Columbia
- **Focus:** Automated drafting of ultrasound narratives with trustworthy language models.
- Coupled ViT/BLIP vision encoders with T5 generation heads to produce structured, clinician-ready summaries.
- Curated DICOM-compliant datasets, performed careful de-identification, and integrated saliency overlays for interpretability.
- Collaborated with radiologists to benchmark BLEU, ROUGE-L, and clinical efficacy metrics before deployment.

## Texture-free motion intelligence
- **Lab:** L3S Research Center
- **Focus:** Domain-robust action understanding free from scene texture bias.
- Designed optical-flow-first datasets and augmentations that emphasise temporal cues over backgrounds.
- Trained hybrid 3D CNN and transformer models to generalise across lighting, texture, and capture conditions.
- Analysed attention maps and counterfactuals to explain decisions for reviewers and industry partners.

## Open collaboration
I maintain reproducible codebases for each program—agentic evaluation harnesses, data cards, and experiment logs live on GitHub—and I'm eager to co-author papers that explore multimodal alignment, affective computing, and applied clinical AI. If your lab or company is building in these areas, please [reach out](/contact/). I'm actively looking for graduate labs to join, visiting researcher invitations, and joint dataset or benchmark initiatives.
