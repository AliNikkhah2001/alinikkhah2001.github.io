---
title: Advanced Methods for Improving Ultrasound Report Generation
tags: []
published: true
date: '2025-03-18'
---



## 1. Removing Text/Markers with Generative Inpainting

Modern approaches treat the removal of text, calipers, and other non-anatomical markings as an image inpainting problem. Deep generative models like GANs and diffusion models have been successful in blind inpainting – filling in masked regions without explicit location annotations.

For example, a Pyramid GAN was used to remove crosshair markers in thyroid ultrasound images by leveraging multi-scale context [1]. Diffusion models with appropriate conditioning (e.g. using ControlNet or masked diffusion) can similarly erase text overlays by treating text removal as a conditional generation task [2]. These methods learn to synthesize realistic tissue texture where the text or artifact was, producing a clean image.

When deep learning resources are limited, classical inpainting techniques are still relevant: methods like patch-based filling (e.g. PatchMatch) or partial convolution can propagate neighboring pixel intensities into text regions. A morphology-based strategy solves a Laplacian equation on the image domain to seamlessly fill marker regions – essentially a fast interpolation guided by surrounding pixel values [3].

Traditional approaches, such as patch-based matching and interpolation, can remove simple annotations without learning, although they may struggle with large or complex obstructions.

## 2. Standardizing Image Size and Shape

Ultrasound images often come in varying sizes or with irregular “fan” shapes (sector scan regions) and padding. To feed data consistently to deep models, preprocessing pipelines use transformations to normalize geometry.

A straightforward approach is to pad and resize all images to a fixed resolution, preserving aspect ratio by adding black borders. However, more advanced techniques avoid over-cropping or warping important anatomy. Spatial transformer networks (STN) can automatically learn to rotate, scale, or crop images to a canonical view [4].

For example, an STN module can be trained to detect the organ of interest and align it to the center of the frame, without any manual ROI annotations. Another strategy is coordinate warping based on ultrasound physics: fan-shaped scans can be converted to a rectangular grid by re-sampling along the polar angle, applying standard augmentations, then warping back [5].

Combining automated cropping (or STN alignment) with appropriate padding yields uniform image inputs without requiring predefined crop regions. These steps produce a high-quality, standardized dataset as a foundation for report generation models.

# Unsupervised Report Clustering and Topic Discovery

## 1. Clustering and Topic Modeling of Textual Reports

Unsupervised learning can structure free-text ultrasound reports into meaningful groups, which helps distill common findings or narratives. A typical pipeline embeds each report into a numerical vector space (using techniques like TF-IDF, doc2vec, or Sentence-BERT) and then applies clustering on these embeddings [6].

For instance, one approach converted the text of ultrasound reports to embeddings (Sentence-BERT yielded good representations for long medical text), reduced dimensionality (via UMAP), and then clustered with k-means to uncover $K$ latent topics [7]. Each cluster corresponds to a set of reports with similar content.

Wang et al. demonstrated that such unsupervised clustering can identify the underlying major topics present in nearly 1.9 million radiology reports [8]. Likewise, topic modeling algorithms like Latent Dirichlet Allocation (LDA) have been applied to radiology text to automatically learn themes such as “lesion description,” “normal exam,” or “Doppler findings.”

## 2. Linking Image Regions to Text Clusters

Once report clusters or topics are defined, a next-level challenge is associating aspects of the images with those topics. Multi-modal learning techniques use weakly supervised alignment to connect textual patterns with image regions.

For example, a model can learn attention weights that highlight which part of an ultrasound image corresponds to a particular cluster’s content. Attention-based architectures can also create explicit linkages using co-attention or cross-modal attention. Another approach uses contrastive learning across image patches and sentence embeddings to align visual patterns with report topics [9].

In summary, unsupervised textual clustering (using methods like k-means or LDA) yields high-level groupings of report content, and new deep learning techniques can associate those groupings with image evidence.

# Alternative Loss Functions and Model Improvements

## 1. Beyond Triplet Loss – Contrastive and Self-Supervised Learning

Many current ultrasound report generators use combined objectives (e.g. image-text matching losses like triplet loss). Contrastive learning pulls matched image–text pairs together in the feature space and pushes mismatched pairs apart. A prominent example is the ConVIRT framework, which pretrains medical image encoders using a bidirectional image-text contrastive loss [10].

Beyond image-text contrast, self-supervised losses on individual modalities can also help. For images, one could incorporate losses for predicting transformations (e.g. rotation, inversion) or masked patch reconstruction. For text, language-modeling losses (predicting masked words or next sentence) could be added.

## 2. Reinforcement Learning for Report Generation

Another approach formulates report generation as a reinforcement learning (RL) problem, where the model’s “action” is to output a sequence of words and a reward function measures the quality or accuracy of the report. Qin and Song introduced an RL agent that learns to align visual and textual features via a reward signal from a cross-modal memory [11].

RL can also use external metrics as feedback. One study used a clinical fact accuracy metric (RadGraph’s F1 score) as part of the reward to penalize factual errors in generated reports [12]. Similarly, self-critical sequence training has been used, where the reward is based on domain metrics like BLEU or specialized medical scores.

# Ensuring Medical Accuracy and Consistency

## 1. Automated Fact-Checking with Knowledge Graphs and Rules

To verify the medical accuracy of generated reports, researchers are turning to explicit knowledge-based systems. One approach is to use a medical knowledge graph (KG) – a structured database of medical facts – as a reference for checking the report [13].

Rule-based systems are another method. If the report mentions “gallbladder: normal” it should not also mention gallstones, or if a measurement is given, it must be within physiological limits. While no rule set can cover every scenario, even a small library of rules can catch blatant errors.

## 2. Secondary Model Verification and Expert-in-the-Loop

Another line of defense is using independent AI models or classifiers to validate the report’s content against the images. For instance, after a report is generated, a secondary model examines the ultrasound image and predicts key findings. If the report contradicts the image model’s output, it can trigger a warning [14].

In practical deployment, an expert-in-the-loop can be incorporated at critical points, ensuring human oversight in high-stakes clinical settings.

# References

1. Automatic consecutive context perceived transformer GAN for serial inpainting.
2. DiffSTR: Controlled Diffusion Models for Scene Text Removal.
3. Morphology-Based Inpainting for Removal of Markers in Medical Images.
4. Automatic Facial Axes Standardization of 3D Fetal Ultrasound Images.
5. UltraAugment: Fan-shape and Artifact-based Data Augmentation for 2D Ultrasound Images.
6. Methodology for Unsupervised Radiology Report Clustering.
7. Large-Scale Topic Modeling in Radiology Reports.
8. Unsupervised Topic Modeling in a Large Free Text Radiology Report Repository.
9. Multi-Grained Radiology Report Generation with Sentence-Level Image-Language Contrastive Learning.
10. Contrastive Learning of Medical Visual Representations from Paired Images and Text.
11. Reinforced Cross-modal Alignment for Radiology Report Generation.
12. Improving Radiology Report Generation Quality and Diversity through Reinforcement Learning and Text Augmentation.
13. Knowledge Graph Applications in Medical Imaging Analysis.
14. Levels of Autonomous Radiology in Medical Imaging.

